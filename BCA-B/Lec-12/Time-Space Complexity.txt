/*
==================== TIME COMPLEXITY ====================

Definition:
➡ Time Complexity is the amount of time taken by an algorithm 
   as a function of the size of input (n).

Why Needed?
➡ Helps to analyze and compare efficiency of algorithms
   without actually running them.

Types of Notations:
1. Big-O (O): Worst case
   - Upper bound on running time
   - Example: Linear Search → O(n)

2. Omega (Ω): Best case
   - Lower bound on running time
   - Example: Linear Search (element at first index) → Ω(1)

3. Theta (Θ): Average case
   - Tight bound (both upper and lower)
   - Example: Linear Search average → Θ(n/2) = Θ(n)

Common Complexities (from Fastest to Slowest):
- O(1)   → Constant time
- O(log n) → Logarithmic
- O(n)   → Linear
- O(n log n) → Linearithmic
- O(n^2) → Quadratic
- O(2^n) → Exponential
- O(n!)  → Factorial

Example:
for(int i = 0; i < n; i++) {
   cout << i;
}
Time Complexity = O(n)

---------------------------------------------------------

==================== SPACE COMPLEXITY ===================

Definition:
➡ Space Complexity is the amount of memory required by an 
   algorithm as a function of input size (n).

Components of Space:
1. Fixed part
   - Memory for instructions, constants, variables.
2. Variable part
   - Memory for dynamic allocation, recursion stack, arrays, etc.

Examples:
- int arr[1000]; → Uses O(1000) = O(n) space
- int x; → Uses O(1) space

---------------------------------------------------------

==================== COMPARISON TABLE ===================

| Complexity Type | Definition                         | Example                     |
|-----------------|-----------------------------------|-----------------------------|
| Time Complexity | Measures execution time growth    | Linear search → O(n)        |
| Space Complexity| Measures memory growth            | Array of size n → O(n)      |

*/





// notations 




/*
==================== 1. WHY TIME COMPLEXITY? ====================

- Computers execute millions of operations per second.
- But as input size (n) grows, execution time also grows.
- Instead of measuring time in seconds (depends on hardware), 
  we analyze *growth rate* using Time Complexity.

---------------------------------------------------------------

==================== 2. BIG-O NOTATION (O) =====================

- Represents the **worst case** (upper bound).
- Tells us how fast time grows when n increases.

Example 1: O(1) → Constant Time
--------------------------------
void printFirst(int arr[]) {
    cout << arr[0];   // Always 1 operation
}
// Time doesn't depend on n.

Example 2: O(n) → Linear Time
--------------------------------
void printAll(int arr[], int n) {
    for(int i = 0; i < n; i++) {
        cout << arr[i] << " ";
    }
}
// Loop runs n times → O(n)

Example 3: O(n^2) → Quadratic Time
--------------------------------
void printPairs(int arr[], int n) {
    for(int i = 0; i < n; i++) {
        for(int j = 0; j < n; j++) {
            cout << arr[i] << "," << arr[j] << " ";
        }
    }
}
// Nested loop → n * n = O(n^2)

---------------------------------------------------------------

==================== 3. OMEGA NOTATION (Ω) ====================

- Represents the **best case** (lower bound).
- The least number of steps the algorithm will take.

Example: Linear Search
--------------------------------
bool search(int arr[], int n, int key) {
    for(int i = 0; i < n; i++) {
        if(arr[i] == key) return true;
    }
    return false;
}
// Best case → key found at index 0 → Ω(1)
// Worst case → key not found → O(n)

---------------------------------------------------------------

==================== 4. THETA NOTATION (Θ) ====================

- Represents the **average case** (tight bound).
- On average, how much time does it take?

Example: Linear Search (average case)
--------------------------------
On average, key will be found in middle → n/2 steps
So Time Complexity = Θ(n)

---------------------------------------------------------------

==================== 5. COMMON TIME COMPLEXITIES ===============

| Complexity   | Example Code / Algorithm                     |
|--------------|----------------------------------------------|
| O(1)         | Accessing array element: arr[i]              |
| O(log n)     | Binary Search                                |
| O(n)         | Linear Search, traversing an array           |
| O(n log n)   | Merge Sort, Quick Sort (average case)        |
| O(n^2)       | Nested loops, Bubble Sort, Insertion Sort    |
| O(2^n)       | Recursion on subsets (e.g., Subset problems) |
| O(n!)        | Travelling Salesman Problem (Brute force)    |

---------------------------------------------------------------

==================== 6. ADVANCED INSIGHTS =====================

1. Dropping Constants:
   O(2n) → O(n)
   O(3n^2 + 5n + 10) → O(n^2)

2. Considering Dominant Term:
   For T(n) = n^2 + n → O(n^2)

3. Logarithmic Complexity (O(log n)):
   - Happens when input is divided into halves.
   Example: Binary Search

int binarySearch(vector<int>& arr, int key) {
    int low = 0, high = arr.size() - 1;
    while(low <= high) {
        int mid = (low + high) / 2;
        if(arr[mid] == key) return mid;
        else if(arr[mid] < key) low = mid + 1;
        else high = mid - 1;
    }
    return -1;
}
// Each step reduces input size by half → O(log n)

---------------------------------------------------------------

==================== 7. VISUAL HIERARCHY ======================

Fastest → Slowest Growth (when n increases):

O(1) < O(log n) < O(n) < O(n log n) < O(n^2) < O(2^n) < O(n!)

*/
